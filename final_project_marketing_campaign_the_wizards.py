# -*- coding: utf-8 -*-
"""Final Project Marketing Campaign - The Wizards.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_rq0fm9QOBmm9cv_GpXjFfZU_pXUL4FG

# Marketing Campaign

The Wizards (Kelompok 7):
- Dwi Abriansya Alimuddin
- Ni Kadek Rina Wati
- Almaira Nabila Ayudhiya
- Ivander Edo
- Naufal Fajar Revanda

## Context
A response model can provide a significant boost to the efficiency of a marketing campaign by increasing responses or reducing expenses. The objective is to predict who will respond to an offer for a product or service

### Content
AcceptedCmp1 - 1 if customer accepted the offer in the 1st campaign, 0 otherwise
<br>AcceptedCmp2 - 1 if customer accepted the offer in the 2nd campaign, 0 otherwise
<br>AcceptedCmp3 - 1 if customer accepted the offer in the 3rd campaign, 0 otherwise
<br>AcceptedCmp4 - 1 if customer accepted the offer in the 4th campaign, 0 otherwise
<br>AcceptedCmp5 - 1 if customer accepted the offer in the 5th campaign, 0 otherwise
<br>Response (target) - 1 if customer accepted the offer in the last campaign, 0 otherwise
<br>Complain - 1 if customer complained in the last 2 years
<br>DtCustomer - date of customer’s enrolment with the company
<br>Education - customer’s level of education
<br>Marital - customer’s marital status
<br>Kidhome - number of small children in customer’s household
<br>Teenhome - number of teenagers in customer’s household
<br>Income - customer’s yearly household income
<br>MntFishProducts - amount spent on fish products in the last 2 years
<br>MntMeatProducts - amount spent on meat products in the last 2 years
<br>MntFruits - amount spent on fruits products in the last 2 years
<br>MntSweetProducts - amount spent on sweet products in the last 2 years
<br>MntWines - amount spent on wine products in the last 2 years
<br>MntGoldProds - amount spent on gold products in the last 2 years
<br>NumDealsPurchases - number of purchases made with discount
<br>NumCatalogPurchases - number of purchases made using catalogue
<br>NumStorePurchases - number of purchases made directly in stores
<br>NumWebPurchases - number of purchases made through company’s web site
<br>NumWebVisitsMonth - number of visits to company’s web site in the last month
<br>Recency - number of days since the last purchase
<br>Z_CostContact - Cost Campaign per Customer
<br>Z_Revenue - Revenue Campaign per Customer

Acknowledgements
<br>O. Parr-Rud. Business Analytics Using SAS Enterprise Guide and SAS Enterprise Miner. SAS Institute, 2014.

Inspiration
<br>The main objective is to train a predictive model which allows the company to maximize the profit of the next marketing campaign.
"""

# Commented out IPython magic to ensure Python compatibility.
import warnings
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
pd.set_option('display.max_columns', None)

sns.set(rc={'figure.figsize': (20.7,8.27)})
sns.set_style("whitegrid")
sns.color_palette("dark")
plt.style.use("fivethirtyeight")

"""## Load Dataset"""

from google.colab import files
uploaded = files.upload()

# Load Dataset
df = pd.read_csv('marketing_campaign.csv', sep = ';')
df.head(10)

"""- `Year_Birth` diubah menjadi `age`
- `Marital_Status` di regroup menjadi 2 value saja
- `Kidhome` & `Teenhome` digabung menjadi jumlah anak
- `Dt_Customer` diubah jadi udah berapa tahun enroll
- `AcceptedCmp1-5` digabungkan menjadi total accepted campaign
- `Z_CostContact` & `Z_Revenue` value sama tiap baris sebaiknya didrop
- Kolom `Response` merupakan kolom target

## Describe Data
"""

df.info()

"""- Terdapat nilai `Null` pada kolom `Income`
- Kolom `Dt_Customer` memiliki tipe data object, diubah menjadi datetime

"""

# Pemisahan kolom berdasarkan data numeric dan category
cats = ['Education', 'Marital_Status']

cat_num = ['Kidhome', 'Teenhome', 'AcceptedCmp1', 'AcceptedCmp2',
           'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5',
           'Complain']

nums = ['Income', 'Recency', 'MntWines', 'MntFruits', 'MntMeatProducts',
        'MntFishProducts', 'MntSweetProducts', 'MntGoldProds',
        'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases',
        'NumStorePurchases', 'NumWebVisitsMonth', 'Year_Birth']

# Statistika deskriptif kolom numerik
df[nums].describe()

"""- Kolom `Income`, `Recency`, dan `Year_Birth` relatif normal karena mean hampir sama dengan median
- `MntWines`, `MntFruits`, `MntMeatProducts`, `MntFishProducts`, `MntSweetProducts`, `MntGoldProds` memiliki nilai mean yang cukup berbeda dari nilai median yang mengindikasikan adanya skewness
"""

# Statistika deskriptif kolom category
df[cats].describe()

"""- `Education` yang paling banyak adalah Graduation dengan frekuensi 1127
- `Marital_Status` yang paling banyak adalah Married dengan frekuensi 864
- Kolom `Marital_Status` memiliki value yang berarti sama, dapat digabung menjadi satu value baru
"""

# Ekstrak variable cost dan revenue dari DataFrame
cost = df['Z_CostContact'].unique()[0]
revenue = df['Z_Revenue'].unique()[0]

"""### Campaign 1"""

# Melihat hasil Campaign pertama
df['AcceptedCmp1'].value_counts()

# Menghitung Conversion Rate dan ROI dari Campaign pertama
convert_cmp_1 = df['AcceptedCmp1'].value_counts()[1]
not_convert_cmp_1 = df['AcceptedCmp1'].value_counts()[0]
total_cmp_1 = convert_cmp_1 + not_convert_cmp_1

conv_rate_cmp_1 = (convert_cmp_1/total_cmp_1)*100
ROI_cmp_1 = ((convert_cmp_1*revenue)/(total_cmp_1*cost))*100

print(('Conversion Rate Campaign 1: %.2f' % conv_rate_cmp_1)+'%')
print(('Return Of Investment Campaign 1: %.2f' % ROI_cmp_1)+'%')

"""### Campaign 2"""

# Melihat hasil Campaign kedua
df['AcceptedCmp2'].value_counts()

# Menghitung Conversion Rate dan ROI dari Campaign kedua
convert_cmp_2 = df['AcceptedCmp2'].value_counts()[1]
not_convert_cmp_2 = df['AcceptedCmp2'].value_counts()[0]
total_cmp_2 = convert_cmp_2 + not_convert_cmp_2

conv_rate_cmp_2 = (convert_cmp_2/total_cmp_2)*100
ROI_cmp_2 = ((convert_cmp_2*revenue)/(total_cmp_2*cost))*100

print(('Conversion Rate Campaign 2: %.2f' % conv_rate_cmp_2)+'%')
print(('Return Of Investment Campaign 2: %.2f' % ROI_cmp_2)+'%')

"""### Campaign 3"""

# Melihat hasil Campaign ketiga
df['AcceptedCmp3'].value_counts()

# Menghitung Conversion Rate dan ROI dari Campaign ketiga
convert_cmp_3 = df['AcceptedCmp3'].value_counts()[1]
not_convert_cmp_3 = df['AcceptedCmp3'].value_counts()[0]
total_cmp_3 = convert_cmp_3 + not_convert_cmp_3

conv_rate_cmp_3 = (convert_cmp_3/total_cmp_3)*100
ROI_cmp_3 = ((convert_cmp_3*revenue)/(total_cmp_3*cost))*100

print(('Conversion Rate Campaign 3: %.2f' % conv_rate_cmp_3)+'%')
print(('Return Of Investment Campaign 3: %.2f' % ROI_cmp_3)+'%')

"""### Campaign 4"""

# Melihat hasil Campaign keempat
df['AcceptedCmp4'].value_counts()

# Menghitung Conversion Rate dan ROI dari Campaign keempat
convert_cmp_4 = df['AcceptedCmp4'].value_counts()[1]
not_convert_cmp_4 = df['AcceptedCmp4'].value_counts()[0]
total_cmp_4 = convert_cmp_4 + not_convert_cmp_4

conv_rate_cmp_4 = (convert_cmp_4/total_cmp_4)*100
ROI_cmp_4 = ((convert_cmp_4*revenue)/(total_cmp_4*cost))*100

print(('Conversion Rate Campaign 4: %.2f' % conv_rate_cmp_4)+'%')
print(('Return Of Investment Campaign 4: %.2f' % ROI_cmp_4)+'%')

"""### Campaign 5"""

# Melihat hasil Campaign kelima
df['AcceptedCmp5'].value_counts()

convert_cmp_5 = df['AcceptedCmp5'].value_counts()[1]
not_convert_cmp_5 = df['AcceptedCmp5'].value_counts()[0]
total_cmp_5 = convert_cmp_5 + not_convert_cmp_5

conv_rate_cmp_5 = (convert_cmp_5/total_cmp_5)*100
ROI_cmp_5 = ((convert_cmp_5*revenue)/(total_cmp_5*cost))*100

print(('Conversion Rate Campaign 5: %.2f' % conv_rate_cmp_5)+'%')
print(('Return Of Investment Campaign 5: %.2f' % ROI_cmp_5)+'%')

"""### Last Campaign"""

# Melihat hasil Campaign terakhir
df['Response'].value_counts()

# Menghitung Conversion Rate dan ROI dari Campaign terakhir
convert_last_cmp = df['Response'].value_counts()[1]
not_convert_last_cmp = df['Response'].value_counts()[0]
total_last_cmp = convert_last_cmp + not_convert_last_cmp

conv_rate_last_cmp = (convert_last_cmp/total_last_cmp)*100
ROI_last_cmp = ((convert_last_cmp*revenue)/(total_last_cmp*cost))*100

print(('Conversion Rate Last Campaign: %.2f' % conv_rate_last_cmp)+'%')
print(('Return Of Investment Last Campaign: %.2f' % ROI_last_cmp)+'%')

# Menyatukan hasil seluruh Campaign ke dalam DataFrame
dict_cmp = {'Campaign': ['Campaign 1', 'Campaign 2', 
                         'Campaign 3', 'Campaign 4',
                         'Campaign 5', 'Last Campaign',],
            'Conversion Rate (%)': [conv_rate_cmp_1, conv_rate_cmp_2,
                                    conv_rate_cmp_3, conv_rate_cmp_4,
                                    conv_rate_cmp_5, conv_rate_last_cmp],
            'Return Of Investment (%)': [ROI_cmp_1, ROI_cmp_2,
                                         ROI_cmp_3, ROI_cmp_4,
                                         ROI_cmp_5, ROI_last_cmp]
            } 

df_cmp = pd.DataFrame.from_dict(dict_cmp)
df_cmp

"""## Data Visualization

### ROI 6 Campaign Sebelumnya
"""

# Visualisasi ROI dari 6 Campaign sebelumnya
df_cmp = df_cmp['Return Of Investment (%)']
df_cmp.index = [f'{i}' for i in range(1, 7)]
df_cmp.sort_values('index')

ax = df_cmp.plot(kind = 'bar', color = '#5b7efa', 
                 alpha = .95, fontsize = 13, 
                 figsize = (8, 5))

plt.title('Return Of Investment')
plt.ylabel('ROI (%)')

# Add this loop to add the annotations
for p in ax.patches:
    width = p.get_width()
    height = p.get_height()
    perc = p.get_height()
    x, y = p.get_xy()
    ax.annotate(f'{perc: .2f}%', (x + width/2, y + height*1.01), ha = 'center')

plt.xlabel('Campaign')
plt.legend
plt.xticks(rotation = 0)
plt.ylim(0, 110)
plt.tight_layout()
plt.show()

"""- Selama 6 Campaign terakhir, ROI tidak pernah mencapai 100% yang berarti Campaign selalu Loss

### Hasil Last Campaign
"""

# Visualisasi hasil dari Last Campaign
df_vis = df.copy()
df_vis['Response'] = df_vis['Response'].replace([0, 1], ['Did not accept the last campaign', 'Accepted the last campaign'])
dfresponse = df_vis.groupby(['Response'])['ID'].nunique()

pie, ax = plt.subplots(figsize = [10,6])
labels = dfresponse.keys()
plt.pie(x = dfresponse, autopct = "%.1f%%", explode = [0.05]*2, colors = (['#5b7efa', 'gray']), labels = labels, pctdistance = 0.5, textprops = {'fontsize': 14})
plt.title('Response')

"""- Dari 2240, hanya 334 Customer yang menerima Campaign terakhir.

### Customer's Recency
"""

# Visualisasi Recency Customer dan hubungannya dengan Response
df_vis['Recency_Group'] = np.where(df_vis['Recency'] > 75, '> 75 Days',
                               np.where(df_vis['Recency'] < 25, '< 25 Days', '25-75 Days'))

def response_bar_plot(df, column, title = '', xlabel = '', figsize = (10, 5), ylim_offset = 5, ax = None):
    df_bar = (df.groupby(['Response', column])[
              'ID'].nunique().to_frame('customer_percentage')/df.shape[0])*100
    df_bar = df_bar.reset_index().sort_values(by = column, ascending = True)
    
    
    plt.figure(figsize = figsize)
    
    ax = sns.barplot(x = column, y = 'customer_percentage', hue = 'Response',
                     hue_order = ['Did not accept the last campaign',
                                  'Accepted the last campaign'],
                     data = df_bar, palette = (['gray', '#5b7efa']), alpha = 1, saturation = 1, edgecolor = 'k', linewidth = .7,
                     order=['< 25 Days', '25-75 Days', '> 75 Days'])
    
    
    # Add this loop to add the annotations
    for p in ax.patches:
        width = p.get_width()
        height = p.get_height()
        x, y = p.get_xy()
        ax.annotate('%.1f' % height + '%', (x + width /
                                            1.8, y + height*1.01), ha = 'center')

    ax.xaxis.set_tick_params(labelsize = 15)
    ax.yaxis.set_tick_params(labelsize = 15)
    plt.legend(fontsize = 'medium', loc = 'best', title = '')
    plt.title(title)
    plt.xlabel(xlabel)
    plt.ylabel('% customers')
    plt.ylim(0, df_bar.customer_percentage.max() + ylim_offset)
    plt.tight_layout()

response_bar_plot(df_vis, 'Recency_Group', title = 'Recency')

"""- Persentase tertinggi customer yang menerima Campaign terakhir berasal dari pelanggan yang melakukan pembelian terakhir mereka tidak lebih dari 75 hari yang lalu.

### Products and Channels
"""

# Inisiasi fungsi Visualisasi
def response_bar_plot2(df, column, title = '', xlabel = '', figsize = (10, 5), ylim_offset = 5, ax = None):
    df_bar = (df.groupby(['Response', column])['Transactions'].sum().to_frame('customer_percentage')/(df['Transactions'].sum()))*100
    df_bar = df_bar.reset_index().sort_values(by = column, ascending = True)
    
    
    plt.figure(figsize = figsize)
    
    ax = sns.barplot(x = column, y = 'customer_percentage', hue = 'Response',
                     hue_order = ['Did not accept the last campaign',
                                  'Accepted the last campaign'],
                     data = df_bar, palette = (['gray', '#5b7efa']), alpha = 1, saturation = 1, edgecolor = 'k', linewidth = .7)
    
    
    # Add this loop to add the annotations
    for p in ax.patches:
        width = p.get_width()
        height = p.get_height()
        x, y = p.get_xy()
        ax.annotate('%.1f' % height + '%', (x + width /
                                            1.8, y + height*1.01), ha = 'center')

    ax.xaxis.set_tick_params(labelsize = 15)
    ax.yaxis.set_tick_params(labelsize = 15)
    plt.legend(fontsize = 'medium', loc = 'best', title = '')
    plt.title(title)
    plt.xlabel(xlabel)
    plt.ylabel('% customers')
    plt.ylim(0, df_bar.customer_percentage.max() + ylim_offset)
    plt.tight_layout()

# Visualisasi Products dan hubungannya dengan Response
dfprods = df_vis[['ID', 'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']]
dfprods = dfprods.rename(columns = {'MntWines': 'Wine', 'MntFruits': 'Fruit',
                                   'MntMeatProducts': 'Meat', 'MntFishProducts': 'Fish',
                                   'MntSweetProducts': 'Sweet', 'MntGoldProds': 'Gold'})
dfprods = dfprods.melt('ID', var_name = 'Products', value_name = 'Transactions')

df2 = df_vis.merge(dfprods, on = 'ID')
df2 = df2[['ID', 'Products', 'Transactions', 'Response']]

# Memanggil fungsi visualisasi untuk visualisasi products
response_bar_plot2(df2, 'Products', title = 'Products')

"""- Customer yang membeli produk Meat dan Wine cenderung menerima Campaign terakhir daripada customer yang membeli kategori produk lain."""

# Visualisasi Channels dan hubungannya dengan Response
dfchannels = df_vis[['ID', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']]
dfchannels = dfchannels.rename(columns = {'NumWebPurchases': 'Website', 'NumCatalogPurchases': 'Catalog', 'NumStorePurchases': 'Store'})
dfchannels = dfchannels.melt('ID', var_name = 'Channels', value_name = 'Transactions')

df3 = df_vis.merge(dfchannels, on = 'ID')
df3 = df3[['ID', 'Channels', 'Transactions', 'Response']]

# Memanggil fungsi Visualisasi untuk visualisasi Channels
response_bar_plot2(df3, 'Channels', title = 'Channels')

"""- Persentase pembelian customer yang menerima Campaign terakhir tidak terlalu berbeda di ketiga channel penjualan."""

# Visualisasi Total Penjualan Products dan Channels
f, ax = plt.subplots(1, 2, figsize = (16, 5))

(df_vis[df_vis['Response'] == 'Accepted the last campaign'][['MntWines', 'MntFruits', 
                       'MntMeatProducts', 'MntFishProducts',
                       'MntSweetProducts', 'MntGoldProds']]\
 .sum()/1000).sort_values().plot(kind = 'barh', 
                                 color = '#5b7efa', 
                                 alpha = .95, 
                                 ax = ax[1])
ax[1].xaxis.set_tick_params(labelsize = 14)
ax[1].yaxis.set_tick_params(labelsize = 14)
ax[1].set_xlabel('Quantity in thousands')
ax[1].set_title('Quantity that customers bought from a product category')

df_vis[df_vis.Response == 'Accepted the last campaign'][['NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']]\
                .sum().sort_values().plot(kind = 'barh', 
                                          color = '#5b7efa', 
                                          alpha = .95, 
                                          ax = ax[0])
ax[0].xaxis.set_tick_params(labelsize = 14)
ax[0].yaxis.set_tick_params(labelsize = 14)
ax[0].set_xlabel('Quantity')
ax[0].set_title("Customer's usage of different sales channels")
plt.tight_layout()
plt.show()

"""-  Sebagian besar pembelian dilakukan melalui store (> 2000 pembelian), diikuti oleh website dan katalog.
- Produk yang paling banyak dibeli oleh customer adalah wine dan meat.

### Campaign Success Rate
"""

# Visualisasi Success Rate dari 6 Campaign sebelumnya
df_vis['Accepted_Campaign'] = df_vis['AcceptedCmp1'] + df_vis['AcceptedCmp2'] + df_vis['AcceptedCmp3'] + df_vis['AcceptedCmp4'] + df_vis['AcceptedCmp5']

def response_bar_plot1(df, column, title = '', xlabel = '', figsize = (10, 5), ylim_offset = 5, ax = None):
    df_bar = (df.groupby(['Response', column])[
              'ID'].nunique().to_frame('customer_percentage')/df.shape[0])*100
    df_bar = df_bar.reset_index().sort_values(by = column, ascending = True)
    
    
    plt.figure(figsize = figsize)
    
    ax = sns.barplot(x = column, y = 'customer_percentage', hue = 'Response',
                     hue_order = ['Did not accept the last campaign',
                                  'Accepted the last campaign'],
                     data = df_bar, palette = (['gray', '#5b7efa']), alpha = 1, saturation = 1, edgecolor = 'k', linewidth = .7)
    
    
    # Add this loop to add the annotations
    for p in ax.patches:
        width = p.get_width()
        height = p.get_height()
        x, y = p.get_xy()
        ax.annotate('%.1f' % height + '%', (x + width /
                                            1.8, y + height*1.01), ha = 'center')

    ax.xaxis.set_tick_params(labelsize = 15)
    ax.yaxis.set_tick_params(labelsize = 15)
    plt.legend(fontsize = 'medium', loc = 'best', title = '')
    plt.title(title)
    plt.xlabel(xlabel)
    plt.ylabel('% customers')
    plt.ylim(0, df_bar.customer_percentage.max() + ylim_offset)
    plt.tight_layout()

# Memanggil fungsi visualisasi
response_bar_plot1(df_vis, 'Accepted_Campaign', title = 'Number of Campaigns Accepted')

"""- Semakin sering Campaign diterima oleh Customer, semakin besar kemungkinan mereka akan menerima Campaign berikutnya."""

df_success = df_vis[['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']].sum()
df_success.index = [f'{i}' for i in range(1, 6)]
df_success.sort_values('index')
ax = df_success.plot(kind = 'bar', color = '#5b7efa', 
                 alpha = .95, fontsize = 13, 
                 figsize = (8, 5))

plt.title('Campaign success rate')
plt.ylabel('Number of customers')

# Add this loop to add the annotations
for p in ax.patches:
    width = p.get_width()
    height = p.get_height()
    perc = p.get_height()/df.shape[0]
    x, y = p.get_xy()
    ax.annotate(f'{perc: .02%}', (x + width/2, y + height*1.01), ha = 'center')

plt.xlabel('Campaign')
plt.xticks(rotation = 0)
plt.ylim(0, 370)
plt.tight_layout()
plt.show()

"""- Tiga Campaign terakhir adalah Campaign paling “berhasil” dari kelima Campaign yang diberikan kepada Customer.

## Univariate Analysis
"""

# Menampilkan Boxplot
plt.figure(figsize=(20, 15))
for i in range(0, len(nums)):
    plt.subplot(2, 7, i+1)
    sns.boxplot(y=df[nums[i]], color='blue', orient='v')
    plt.tight_layout()

"""- Kolom `Income`, `Year_Birth`, `MntWines`, `MntFruits`, `MntMeatProducts`, `MntFishProducts`, `MntSweetProducts`, `MntGoldProds`, `NumDealsPurchases`, `NumWebPurchases`, `NumCatalogPurchases`, `NumWebVisitsMonth` terdapat oulier"""

# Menampilkan Distribution plot
plt.figure(figsize=(20, 15))
for i in range(0, len(nums)):
    plt.subplot(2, 7, i+1)
    sns.distplot(df[nums[i]], color='gray')
    plt.tight_layout()

"""- Kolom `Income`, `Recency`, dan `Year_Birth` memiliki distribusi yang relatif normal
- `MntWines`, `MntFruits`, `MntMeatProducts`, `MntFishProducts`, `MntSweetProducts`, `MntGoldProds`, `NumDealsPurchases`, `NumWebPurchases`, `NumCatalogPurchases`, `NumStorePurchases` memiliki distribusi positively skewed
"""

# Menampilkan Violinplot
plt.figure(figsize=(20, 15))
for i in range(0, len(nums)):
    plt.subplot(2, 7, i+1)
    sns.violinplot(y=df[nums[i]], color='gray', orient='v')
    plt.tight_layout()

# Menampilkan distribusi kolom category
for i in range(0, len(cats)):
    plt.subplot(1, len(cats), i+1)
    sns.countplot(df[cats[i]], color='gray', orient='v')
    plt.tight_layout()

"""- `Education` terbanyak adalah Graduation
- `Marital_Status` terbanyak adalah Married
- `Marital_Status` perlu dikelompokkan ulang

## Multivariate Analysis
"""

# Menampilkan korelasi antar kolom menggunakan heatmap
plt.figure(figsize=(20, 20))
sns.heatmap(df.corr(), cmap='Blues', annot=True, fmt='.2f')

"""- Feature `AcceptedCmp5`, `AcceptedCmp3`, `AcceptedCmp1`, `MntWines`, `MntMeatProducts`, `NumCatalogPurchases`, dan `Recency` memiliki korelasi yang relatif tinggi terhadap Response

# Data Preparation

## Missing Value
"""

# Drop mising value
df_clean = df.dropna()

# Cek dataset setelah missing value didrop
df_clean.info()

"""- 24 data `NULL` telah didrop"""

df_clean.isna().sum()

"""## Duplicated Data"""

# Cek data duplicated
df_clean.duplicated().sum()

"""- Tidak terdapat duplicated data

## Outliers
"""

# Masukkan kolom yang memiliki Outlier
filter_IQR = ['Year_Birth', 'Income', 'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts',
              'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases','NumWebPurchases',
              'NumCatalogPurchases','NumWebVisitsMonth']

"""### Filter IQR"""

# Filter Outlier dengan menggunakan IQR
print(f'Jumlah baris sebelum memfilter outlier: {len(df_clean)}')
filtered_entries = np.array([True] * len(df_clean))
for col in filter_IQR:
    Q1 = df_clean[col].quantile(0.25)
    Q3 = df_clean[col].quantile(0.75)
    IQR = Q3 - Q1
    low_limit = Q1 - (IQR * 1.5)
    high_limit = Q3 + (IQR * 1.5)

    filtered_entries = ((df_clean[col] >= low_limit) & (df_clean[col] <= high_limit)) & filtered_entries
    
df_filter = df_clean[filtered_entries]

print(f'Jumlah baris setelah memfilter outlier: {len(df_filter)}')

"""- Sebanyak 710 outlier telah difilter"""

# Menampilkan Distribution plot setelah filter outliers
plt.figure(figsize=(20, 15))
for i in range(0, len(nums)):
    plt.subplot(2, 7, i+1)
    sns.distplot(df_filter[nums[i]], color='gray')
    plt.tight_layout()

"""- Distribusi menjadi semakin jelas setelah outlier difilter

## Log/Exp Transform
"""

# Masukkan kolom yang distribusinya positively skew
log = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts',
       'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases',
       'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']

# Tranformasi menggunakan Log
for col in log:
    df_filter[col] = np.log(df_filter[col]+1)

# Menampilkan Distribution plot setelah transform
plt.figure(figsize=(20, 15))
for i in range(0, len(nums)):
    plt.subplot(2, 7, i+1)
    sns.distplot(df_filter[nums[i]], color='gray')
    plt.tight_layout()

"""- Distribusi menjadi lebih normal (kurang skew) setelah ditransformasi

## Feature Engineering
"""

df_feature = df_filter.copy()

# Lakukan Feature engineering

from datetime import datetime

list_tahun = []
for index, kolom in df_feature.iterrows():
    tahun = datetime.strptime(kolom['Dt_Customer'], '%Y-%m-%d')
    list_tahun.append(tahun)

df_feature['Dt_Customer'] = list_tahun

year_enroll = []
for index, kolom in df_feature.iterrows():
    enroll = int(datetime.strftime(kolom['Dt_Customer'], '%Y'))
    year_enroll.append(enroll)

# Mengubah Dt_Customer menjadi Long_Enroll
df_feature['Dt_Customer'] = year_enroll
df_feature['Long_Enroll'] = 2014 - df_feature['Dt_Customer']

# Mengubah Year_Birth menjadi age
df_feature['age'] = 2014 - df_feature['Year_Birth']

# Menggabungkan Kidhome dan Teenhome menjadi Children
df_feature['Children'] = df_feature['Kidhome'] + df_feature['Teenhome']

# Menggabungkan Marital_Status menjadi 2 value saja
df_feature['Marital_Status'] = np.where(df_feature['Marital_Status'].isin(['Married', 'Together']), 'In a Relationship', 'Single')

# Menggabungkan AcceptedCmp menjadi total accepted campaign
df_feature['acc_cmp'] = df_feature['AcceptedCmp1'] + df_feature['AcceptedCmp2'] + df_feature['AcceptedCmp3'] + df_feature['AcceptedCmp4'] + df_feature['AcceptedCmp5']

# Drop kolom yang telah di feature engineer
df_feature = df_feature.drop(columns=['Year_Birth', 'Z_Revenue', 'Z_CostContact', 'Dt_Customer', 'AcceptedCmp1', 
                                      'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'Kidhome', 'Teenhome'])

# Melihat hasil feature Engineering
df_feature.head()

"""#### Label Encoding"""

# One-Hot Encoding untuk kolom Marital_Status dan Education
for cat in cats:
    onehots = pd.get_dummies(df_feature[cat], prefix = cat)
    df_feature = df_feature.join(onehots)

# Melihat hasil Encoding
df_feature.head()

# Menambahkan kolom hasil encoding ke dalam list cat_num
cat_num.append('Marital_Status_In a Relationship')
cat_num.append('Marital_Status_Single')
cat_num.append('acc_cmp')
cat_num.append('Education_2n Cycle')
cat_num.append('Education_Basic')
cat_num.append('Education_Graduation')
cat_num.append('Education_Master')
cat_num.append('Education_PhD')
cat_num.append('Long_Enroll')
cat_num.append('Children')
cat_num.remove('Kidhome')
cat_num.remove('Teenhome')
cat_num.remove('AcceptedCmp1')
cat_num.remove('AcceptedCmp2')
cat_num.remove('AcceptedCmp3')
cat_num.remove('AcceptedCmp4')
cat_num.remove('AcceptedCmp5')
nums.remove('Year_Birth')
nums.append('age')

"""## Standardisasi"""

# Melakukan dan Standardisasi
from sklearn.preprocessing import MinMaxScaler, StandardScaler
df_std = df_feature.copy()

for cols in (nums+cat_num):
    df_std[cols] = StandardScaler().fit_transform(df_std[cols].values.reshape(len(df_std), 1))

# Melihat hasil Standardisasi
df_std.describe()

"""## Machine Learning Model

### Train Test Split
"""

# Train Test Split dengan Rasio 70:30
from sklearn.model_selection import train_test_split
X = df_std.drop(columns = ['ID', 'Education', 'Marital_Status', 'Response'])
y = df_std[['Response']]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42, stratify=y)

"""#### Balance Data Train"""

y_train.value_counts()

# Lakukan Oversampling SMOTE
from imblearn import under_sampling, over_sampling
X_train, y_train = over_sampling.SMOTE(random_state=42).fit_resample(X_train, y_train)

# Tambahkan kembali column ke X_train yang telah di oversampling
X_train = pd.DataFrame(
    X_train, 
    columns=['Income', 'Recency', 'MntWines', 'MntFruits', 'MntMeatProducts',
             'MntFishProducts', 'MntSweetProducts', 'MntGoldProds',
             'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases',
             'NumStorePurchases', 'NumWebVisitsMonth', 'Complain', 
             'Long_Enroll', 'age', 'Children', 'acc_cmp', 'Education_2n Cycle',
             'Education_Basic', 'Education_Graduation', 'Education_Master', 
             'Education_PhD', 'Marital_Status_In a Relationship', 
             'Marital_Status_Single'])

# Tambahkan kembali column ke y_train yang telah di oversampling
y_train = pd.DataFrame(y_train, columns=['Response'])

y_train.value_counts()

"""### Model Evaluation"""

# Import metrics evaluasi
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, fbeta_score
from sklearn.metrics import roc_curve, auc
from sklearn.model_selection import KFold, cross_val_score

# Membuat fungsi untuk evaluasi model
def eval_classification(model, pred, X_train, y_train, X_test, y_test):
    print("Accuracy (Test Set): %.2f" % accuracy_score(y_test, pred))
    print("Precision (Test Set): %.2f" % precision_score(y_test, pred))
    print("Recall (Test Set): %.2f" % recall_score(y_test, pred))
    print("F1-Score (Test Set): %.2f" % f1_score(y_test, pred))
    print("FBeta-Score (Test Set): %.2f" % fbeta_score(y_test, pred, beta=0.5))
    fpr, tpr, thresholds = roc_curve(y_test, pred, pos_label=1)
    print("AUC: %.2f" % auc(fpr, tpr))

# Membuat fungsi untuk cross validasi
def cross_score(model, x, y, fold, scoring):
    kf = KFold(n_splits=fold, random_state=42)
    results = cross_val_score(model, x, y, cv=kf, scoring=scoring)
    for index, res in enumerate(results):
        print('Performance fold', index+1, '('+scoring+')', ':', res)
    print("\ncross validation score: %.2f" % np.mean(results))

# Membuat fungsi untuk menampilkan feature importance
def show_feature_importance(model):
    feat_importances = pd.Series(model.feature_importances_, index=X.columns)
    ax = feat_importances.nlargest(25).plot(kind='barh', figsize=(10, 8))
    ax.invert_yaxis()

# Membuat fungsi untuk menampilkan hyperparameter terbaik
def show_best_hyperparameter(model, hyperparameters):
    for key, value in hyperparameters.items() :
        print('Best '+key+':', model.get_params()[key])

"""### Classification Algorithm"""

# Import Classifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier

# Import RandomizedSearch untuk hyperparameter tuning
from sklearn.model_selection import RandomizedSearchCV

"""#### Logistic Regression"""

# Inisiasi model logistic regression
LR = LogisticRegression(random_state=42)

# Fitting dan Evaluasi model logistic regression
LR.fit(X_train, y_train)
y_pred = LR.predict(X_test)
eval_classification(LR, y_pred, X_train, y_train, X_test, y_test)

# Menampilkan akurasi model pada data train dan test
print('Train score: %.2f' % LR.score(X_train, y_train)) #accuracy
print('Test score: %.2f' % LR.score(X_test, y_test)) #accuracy

"""##### Hyperparameter Tuning"""

# List Hyperparameters yang akan diuji
penalty = ['l2','l1','elasticnet']
C = [0.0001, 0.001, 0.002] # Inverse of regularization strength; smaller values specify stronger regularization.
hyperparameters = dict(penalty=penalty, C=C)

# Tuning logistic regression dengan RandomizedSearch, cross validation = 5
LR = LogisticRegression(random_state=42)
LR_tuned = RandomizedSearchCV(LR, hyperparameters, cv=5, random_state=42, scoring='accuracy')

# Fitting dan Evaluasi model setelah tuning
LR_tuned.fit(X_train, y_train)
y_pred = LR_tuned.predict(X_test)
eval_classification(LR_tuned, y_pred, X_train, y_train, X_test, y_test)

# Menampilkan akurasi model pada data train dan test
print('Train score: %.2f' % LR_tuned.score(X_train, y_train)) #accuracy
print('Test score: %.2f' % LR_tuned.score(X_test, y_test)) #accuracy

# Menampilkan hyperparameter terbaik
print('Best algorithm:', LR_tuned.best_estimator_.get_params()['penalty'])
print('Best C:', LR_tuned.best_estimator_.get_params()['C'])

"""#### K Nearest Neighbor"""

# Inisiasi model KNN
KNN = KNeighborsClassifier()

# Fitting dan Evaluasi model KNN
KNN.fit(X_train, y_train)
y_pred = KNN.predict(X_test)
eval_classification(KNN, y_pred, X_train, y_train, X_test, y_test)

# Menampilkan akurasi model pada data train dan test
print('Train score: %.2f' % KNN.score(X_train, y_train)) #accuracy
print('Test score: %.2f' % KNN.score(X_test, y_test)) #accuracy

"""##### Hyperparameter Tuning"""

# List hyperparameter yang akan diuji
n_neighbors = list(range(1,30))
p=[1,2]
algorithm = ['auto', 'ball_tree', 'kd_tree', 'brute']
hyperparameters = dict(n_neighbors=n_neighbors, p=p, algorithm=algorithm)

# Tuning KNN dengan RandomizedSearch, cross validation = 5
KNN = KNeighborsClassifier()
KNN_tuned = RandomizedSearchCV(KNN, hyperparameters, cv=5, random_state=42, scoring='accuracy')

# Fitting dan Evaluasi model setelah tuning
KNN_tuned.fit(X_train, y_train)
y_pred = KNN_tuned.predict(X_test)
eval_classification(KNN_tuned, y_pred, X_train, y_train, X_test, y_test)

# Menampilkan akurasi model pada data train dan test
print('Train score: %.2f' % KNN_tuned.score(X_train, y_train)) #accuracy
print('Test score: %.2f' % KNN_tuned.score(X_test, y_test)) #accuracy

# Menampilkan hyperparameter terbaik
print('Best n_neighbors:', KNN_tuned.best_estimator_.get_params()['n_neighbors'])
print('Best p:', KNN_tuned.best_estimator_.get_params()['p'])
print('Best algorithm:', KNN_tuned.best_estimator_.get_params()['algorithm'])

"""#### Decision Tree"""

# Inisiasi model Decision Tree
DT = DecisionTreeClassifier(random_state=42)

# Fitting dan Evaluasi model Decision Tree
DT.fit(X_train,y_train)
y_pred = DT.predict(X_test)
eval_classification(DT, y_pred, X_train, y_train, X_test, y_test)

# Menampilkan akurasi model pada data train dan test
print('Train score: %.2f' % DT.score(X_train, y_train)) #accuracy
print('Test score: %.2f' % DT.score(X_test, y_test)) #accuracy

# Menampilkan feature importance
show_feature_importance(DT)

"""##### Hyperparameter Tuning"""

# List hyperparameter yang akan diuji
max_depth = [int(x) for x in np.linspace(1, 110, num = 30)] # Maximum number of levels in tree
min_samples_split = [2, 5, 10, 100] # Minimum number of samples required to split a node
min_samples_leaf = [1, 2, 4, 10, 20, 50] # Minimum number of samples required at each leaf node
max_features = ['auto', 'sqrt'] # Number of features to consider at every split

hyperparameters = dict(max_depth=max_depth, 
                       min_samples_split=min_samples_split, 
                       min_samples_leaf=min_samples_leaf,
                       max_features=max_features
                      )

# Tuning Decision Tree dengan RandomizedSearch, cross validation = 5
DT = DecisionTreeClassifier(random_state=42)
DT_tuned = RandomizedSearchCV(DT, hyperparameters, cv=5, random_state=42, scoring='accuracy')

# Fitting dan Evaluasi model setelah tuning
DT_tuned.fit(X_train, y_train)
y_pred = DT_tuned.predict(X_test)#Check performa dari model
eval_classification(DT_tuned, y_pred, X_train, y_train, X_test, y_test)

# Menampilkan akurasi model pada data train dan test
print('Train score: %.2f' % DT_tuned.score(X_train, y_train)) #accuracy
print('Test score: %.2f' % DT_tuned.score(X_test, y_test)) #accuracy

# Menampilkan hyperparameter terbaik
print('Best max_depth:', DT_tuned.best_estimator_.get_params()['max_depth'])
print('Best min_samples_split:', DT_tuned.best_estimator_.get_params()['min_samples_split'])
print('Best min_samples_leaf:', DT_tuned.best_estimator_.get_params()['min_samples_leaf'])
print('Best max_features:', DT_tuned.best_estimator_.get_params()['max_features'])

# Menampilkan feature importance
show_feature_importance(DT_tuned.best_estimator_)

"""### Ensenmble Method"""

# Import classifier ensemble method
from sklearn.ensemble import AdaBoostClassifier
from xgboost import XGBClassifier, XGBRegressor
from sklearn.ensemble import RandomForestClassifier

"""#### AdaBoost"""

# Inisiasi model AdaBoost
ab = AdaBoostClassifier(random_state=42)

# Fitting dan Evaluasi model AdaBoost
ab.fit(X_train,y_train)
y_pred = ab.predict(X_test)
eval_classification(ab, y_pred, X_train, y_train, X_test, y_test)

# Menampilkan akurasi model pada data train dan test
print('Train score: %.2f' % ab.score(X_train, y_train)) #accuracy
print('Test score: %.2f' % ab.score(X_test, y_test)) #accuracy

# Menampilkan feature importance
show_feature_importance(ab)

"""##### Hyperparameter Tuning"""

# List hyperparameter yang akan diuji
hyperparameters = dict(n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 100)], # Jumlah iterasi
                       learning_rate = [float(x) for x in np.linspace(start = 0.001, stop = 0.1, num = 100)],  
                       algorithm = ['SAMME', 'SAMME.R']
                      )

# Tuning AdaBoost dengan RandomizedSearch, cross validation = 5
ab = AdaBoostClassifier(random_state=42)
ab_tuned = RandomizedSearchCV(ab, hyperparameters, random_state=42, cv=5, scoring='accuracy')

# Fitting dan Evaluasi model setelah tuning
ab_tuned.fit(X_train,y_train)
y_pred = ab_tuned.predict(X_test)#Check performa dari model
eval_classification(ab_tuned, y_pred, X_train, y_train, X_test, y_test)

# Menampilkan akurasi model pada data train dan test
print('Train score: %.2f' % ab_tuned.score(X_train, y_train)) #accuracy
print('Test score: %.2f' % ab_tuned.score(X_test, y_test)) #accuracy

# Menampilkan hyperparameter terbaik
show_best_hyperparameter(ab_tuned.best_estimator_, hyperparameters)

# Menampilkan feature importance
show_feature_importance(ab_tuned.best_estimator_)

"""#### XGBoost"""

# Inisiasi model XGBoost
xg = XGBClassifier(random_state=42)

# Fitting dan Evaluasi model XGBoost
xg.fit(X_train, y_train)
y_pred = xg.predict(X_test)
eval_classification(xg, y_pred, X_train, y_train, X_test, y_test)

# Menampilkan akurasi model pada data train dan test
print('Train score: %.2f' % xg.score(X_train, y_train)) #accuracy
print('Test score: %.2f' % xg.score(X_test, y_test)) #accuracy

# Menampilkan feature importance
show_feature_importance(xg)

"""##### Hyperparameter Tuning"""

# List hyperparameter yang akan diuji
hyperparameters = {
                    'max_depth' : [int(x) for x in np.linspace(10, 110, num = 11)],
                    'min_child_weight' : [int(x) for x in np.linspace(1, 20, num = 11)],
                    'gamma' : [float(x) for x in np.linspace(0, 1, num = 11)],
                    'tree_method' : ['auto', 'exact', 'approx', 'hist'],

                    'colsample_bytree' : [float(x) for x in np.linspace(0, 1, num = 11)],
                    'eta' : [float(x) for x in np.linspace(0, 1, num = 100)],

                    'lambda' : [float(x) for x in np.linspace(0, 1, num = 11)],
                    'alpha' : [float(x) for x in np.linspace(0, 1, num = 11)]
                    }

# Tuning XGBoost dengan RandomizedSearch, cross validation = 5
xg = XGBClassifier(random_state=42)
xg_tuned = RandomizedSearchCV(xg, hyperparameters, cv=5, random_state=42, scoring='accuracy')

# Fitting dan Evaluasi model setelah tuning
xg_tuned.fit(X_train,y_train)
y_pred = xg_tuned.predict(X_test)#Check performa dari model
eval_classification(xg_tuned, y_pred, X_train, y_train, X_test, y_test)

# Menampilkan akurasi model pada data train dan test
print('Train score: %.2f' % xg_tuned.score(X_train, y_train)) #accuracy
print('Test score: %.2f' % xg_tuned.score(X_test, y_test)) #accuracy

# Menampilkan hyperparameter terbaik
show_best_hyperparameter(xg_tuned.best_estimator_, hyperparameters)

# Menampilkan feature importance
show_feature_importance(xg_tuned.best_estimator_)

"""#### Random Forest"""

# Inisiasi model Random Forest
RF = RandomForestClassifier(random_state=42)

# Fitting dan Evaluasi model Random Forest
RF.fit(X_train,y_train)
y_pred = RF.predict(X_test)
eval_classification(RF, y_pred, X_train, y_train, X_test, y_test)

# Menampilkan akurasi model pada data train dan test
print('Train score: %.2f' % RF.score(X_train, y_train)) #accuracy
print('Test score: %.2f' % RF.score(X_test, y_test)) #accuracy

# Menampilkan feature importance
show_feature_importance(RF)

"""##### Hyperparameter Tuning"""

#List Hyperparameters yang akan diuji
hyperparameters = dict(
                       n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 20)], # Jumlah subtree 
                       bootstrap = [True], # Apakah pakai bootstrapping atau tidak
                       criterion = ['gini','entropy'],
                       max_depth = [int(x) for x in np.linspace(10, 110, num = 11)],  # Maximum kedalaman tree
                       min_samples_split = [int(x) for x in np.linspace(start = 2, stop = 10, num = 5)], # Jumlah minimum samples pada node agar boleh di split menjadi leaf baru
                       min_samples_leaf = [int(x) for x in np.linspace(start = 1, stop = 10, num = 5)], # Jumlah minimum samples pada leaf agar boleh terbentuk leaf baru
                       max_features = ['auto', 'sqrt', 'log2'], # Jumlah feature yg dipertimbangkan pada masing-masing split
                       n_jobs = [-1], # Core untuk parallel computation. -1 untuk menggunakan semua core
                      )

# Tuning Random Forest dengan RandomizedSearch, cross validation = 5
RF = RandomForestClassifier(random_state=42)
RF_tuned = RandomizedSearchCV(RF, hyperparameters, cv=5, random_state=42, scoring='accuracy')

# Fitting dan Evaluasi model setelah tuning
RF_tuned.fit(X_train,y_train)
y_pred = RF_tuned.predict(X_test)#Check performa dari model
eval_classification(RF_tuned, y_pred, X_train, y_train, X_test, y_test)

# Menampilkan akurasi model pada data train dan test
print('Train score: %.2f' % RF_tuned.score(X_train, y_train)) #accuracy
print('Test score: %.2f' % RF_tuned.score(X_test, y_test)) #accuracy

# Menampilkan hyperparameter terbaik
show_best_hyperparameter(RF_tuned.best_estimator_, hyperparameters)

# Menampilkan feature importance
show_feature_importance(RF_tuned.best_estimator_)

"""### Best Algorithm
Algoritma yang menghasilkan model terbaik adalah Random Forest. Hyperparameter tuning dan eliminasi feature importance rendah akan diaplikasikan pada Algoritma Random Forest.
"""

# Eliminasi feature importance rendah
X = df_std.drop(columns = ['ID', 'Education', 'Marital_Status', 'Response', 'Complain'])
y = df_std[['Response']]

# Split data train dan test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42, stratify=y)

# Lakukan Oversampling SMOTE
X_train, y_train = over_sampling.SMOTE(random_state=42).fit_resample(X_train, y_train)

# Tambahkan kembali column ke X_train yang telah di oversampling
X_train = pd.DataFrame(
    X_train, 
    columns=['Income', 'Recency', 'MntWines', 'MntFruits', 'MntMeatProducts',
             'MntFishProducts', 'MntSweetProducts', 'MntGoldProds',
             'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases',
             'NumStorePurchases', 'NumWebVisitsMonth','Long_Enroll', 'age', 
             'Children', 'acc_cmp', 'Education_2n Cycle','Education_Basic', 
             'Education_Graduation', 'Education_Master', 'Education_PhD', 
             'Marital_Status_In a Relationship', 'Marital_Status_Single'])

# Tambahkan kembali column ke y_train yang telah di oversampling
y_train = pd.DataFrame(y_train, columns=['Response'])

# Inisiasi model dengan best hyperparameter
RF_tuned = RandomForestClassifier(
    random_state=42, n_estimators=400, bootstrap=True, 
    criterion='gini', max_depth=110, min_samples_split=4, 
    min_samples_leaf=1, max_features='log2', n_jobs=-1)

# Fitting dan Evaluasi model
RF_tuned.fit(X_train,y_train)
y_pred = RF_tuned.predict(X_test)#Check performa dari model
eval_classification(RF_tuned, y_pred, X_train, y_train, X_test, y_test)

# Menampilkan akurasi model pada data train dan test
print('Train score: %.2f' % RF_tuned.score(X_train, y_train)) #accuracy
print('Test score: %.2f' % RF_tuned.score(X_test, y_test)) #accuracy

# Menampilkan feature importance
show_feature_importance(RF_tuned)

"""# Business Impact"""

# Menyimpan hasil prediksi model ke dalam DataFrame
ypred = pd.DataFrame(y_pred)

# Melihat value dari hasil prediksi
ypred.value_counts()

"""## Confusion Matrix"""

# Menampilkan Confusion Matrix
from sklearn.metrics import confusion_matrix

cf_matrix = confusion_matrix(y_test, y_pred)

group_names = ['True Neg', 'False Pos', 'False Neg', 'True Pos']
group_counts = ["{0:0.0f}".format(value) for value in
                cf_matrix.flatten()]
group_percentages = ["{0:.2%}".format(value) for value in
                     cf_matrix.flatten()/np.sum(cf_matrix)]
labels = [f"{v1}\n{v2}\n{v3}" for v1, v2, v3 in
          zip(group_names, group_counts, group_percentages)]
labels = np.asarray(labels).reshape(2,2)
plt.figure(figsize = (7, 7))
sns.heatmap(cf_matrix, annot = labels, fmt = '', cmap = 'Blues')

"""- Dari hasil pemodelan diperoleh 20 dari 26 customer yang akan menerima Campaign selanjutnya.

## Perhitungan Impact
"""

group_counts

# Menghitung Impact Model Machine Learning
total_customer = int(group_counts[1])+int(group_counts[3])
customer_lead = int(group_counts[3])
cost_next_campaign = cost
revenue_next_campaign = revenue

# ROI y_test
ROI = ((customer_lead*revenue)/(total_customer*cost))*100

print('Total Customer (y_test):', total_customer,'customer')
print('Customer Lead (y_test):', customer_lead,'customer')
print('Cost (y_test):', (total_customer*cost),'$')
print('Revenue (y_test):', (customer_lead*revenue),'$')
print(('Return Of Investment (y_test): %.2f' % ROI)+'%')

"""- Hasil dari perhitungan menunjukkan bahwa penerapan Machine Learning dapat meningkatkan ROI dari 54% menjadi 282%"""

# Perhitungan Impact jika total_customer diperluas sebesar false negative
total_customer = int(group_counts[1]) + int(group_counts[3]) + int(group_counts[2])
customer_lead_worst = int(group_counts[3])
customer_lead_mid = int(group_counts[3]) + int(int(group_counts[2])/2)
customer_lead_best = int(group_counts[3]) + int(group_counts[2])

# ROI y_test
ROI_worst = ((customer_lead_worst*revenue)/(total_customer*cost))*100
ROI_mid = ((customer_lead_mid*revenue)/(total_customer*cost))*100
ROI_best = ((customer_lead_best*revenue)/(total_customer*cost))*100

print('Cost (y_test):', (total_customer*cost),'$')
print('Total Customer (y_test):', total_customer,'customer')
print('------------------------------------')
print('Customer Lead worst case(y_test):', customer_lead_worst,'customer')
print('Revenue worst case (y_test):', (customer_lead_worst*revenue),'$')
print(('ROI worst case (y_test): %.2f' % ROI_worst)+'%')
print('------------------------------------')
print('Customer Lead mid case (y_test):', customer_lead_mid,'customer')
print('Revenue mid case (y_test):', (customer_lead_mid*revenue),'$')
print(('ROI mid case (y_test): %.2f' % ROI_mid)+'%')
print('------------------------------------')
print('Customer Lead best case (y_test):', customer_lead_best,'customer')
print('Revenue best case (y_test):', (customer_lead_best*revenue),'$')
print(('ROI best case (y_test): %.2f' % ROI_best)+'%')

"""- Terlihat bahwa jika total_customer (total Campaign yang disebar) diperluas sebanyak jumlah False Negative, dalam worst case scenario masih menguntungkan Perusahaan."""